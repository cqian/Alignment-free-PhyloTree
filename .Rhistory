<body>
<p>This is an R HTML document. When you click the <b>Knit HTML</b> button a web page will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:</p>
<!--begin.rcode
summary(cars)
end.rcode-->
<p>You can also embed plots, for example:</p>
<!--begin.rcode fig.width=7, fig.height=6
plot(cars)
end.rcode-->
</body>
</html>
<title>Title</title>
<html>
<head>
<html>
<head>
<title>Title</title>
</head>
<body>
<p>This is an R HTML document. When you click the <b>Knit HTML</b> button a web page will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:</p>
<!--begin.rcode
summary(cars)
end.rcode-->
<p>You can also embed plots, for example:</p>
<!--begin.rcode fig.width=7, fig.height=6
plot(cars)
end.rcode-->
</body>
</html>
<html>
<head>
<title>Title</title>
</head>
<body>
<p>This is an R HTML document. When you click the <b>Knit HTML</b> button a web page will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:</p>
<!--begin.rcode
summary(cars)
end.rcode-->
<p>You can also embed plots, for example:</p>
<!--begin.rcode fig.width=7, fig.height=6
plot(cars)
end.rcode-->
</body>
</html>
install.packages(c("boot", "class", "cluster", "foreign", "KernSmooth", "MASS", "Matrix", "nlme", "nnet", "rpart", "spatial", "survival"))
/*** R
#include <Rcpp.h>
using namespace Rcpp;
// This is a simple example of exporting a C++ function to R. You can
// source this function into an R session using the Rcpp::sourceCpp
// function (or via the Source button on the editor toolbar). Learn
// more about Rcpp at:
//
//   http://www.rcpp.org/
//   http://adv-r.had.co.nz/Rcpp.html
//   http://gallery.rcpp.org/
//
// [[Rcpp::export]]
NumericVector timesTwo(NumericVector x) {
return x * 2;
}
// You can include R code blocks in C++ files processed with sourceCpp
// (useful for testing and development). The R code will be automatically
// run after the compilation.
//
/*** R
timesTwo(42)
*/
Rcpp::sourceCpp('Untitled.cpp')
Rcpp::sourceCpp('Untitled.cpp')
clean
rm(ls=list())
rm(list=ls)
rm(list()=ls)
ls
rm(listls())
rm(list=ls())
ls()
install.packages("shiny")
install.packages("ggplot2")
install.packages("sqldf")
install.packages("plry")
install.packages("plyr")
library(shiny)
runExample("01_hello")
ls(pos = 1)
ls(pos = 2)
library(MASS)
ls(pos = 2)
?ls
abbey()
abbey
ls(pos = 0)
ls(pos = 1)
ls(pos = 2)
ls(pos = 3)
ls(pos = 4)
ls(pos = 5)
ls(pos = 6)
ls
ls()
x = [1,2,3]
x = {1,2,3}
x = c(1,2,3)
ls()
x[0]
x[1]
x[2]
a = "sdfsdfsdf"
a[0]
a[1]
a
a(0)
?qnorm
?norm
qnorm(c(1,2,3,4,5),1,2)
qnorm(c(1,2,3,4,5), mean=1,mu=2)
?qnorm
qnorm(c(1,2,3,4,5), mean=1, sd=2)
qnorm(c(1,2,3,4,5), mean=1, sd=1)
qnorm(c(1,2,3,4,5))
x = rt(250, df=9)
?rt
x
qnorm(x)
?qnorm
?qqnorm
qqnorm(x)
?rt
qnorm(0.85,mean=70,sd=3)
?qnorm
dnorm(0.85,mean=70,sd=3)
pnorm(0.85,mean=70,sd=3)
qnorm(abbey)
qnorm(x)
qqnorm(x)
qqnorm(abbey)
?gname
??gname
?name
?rt
?random
?randu
sample(1:10)
sample(1:1)
sample(1:2)
sample(1:10)
?sample
sample(1:2, 10)
sample(1:10, 10)
sample(1:10, 20)
?runif
runif(100)
runif(100,5,10)
libaray(Rcpp)
install.packages(Rcpp)
install.packages("Rcpp")
library(Rcpp)
library("rstan")
library("Rcpp")
library("rstan")
y ~ binomial(n,p);
source('~/gcf.R')
library(rstan)
source('~/gcf.R')
source('~/gcf.R')
source('~/gcf.stan.R')
library("rstan")
x = golf$x
ls
cd ~/
cd ~
ls
ls
golf = read.table("data.txt", header=TRUE, skip=2)
ls
ls()
read.table("data.txt")
read.table("data.txt")
read.table("~/Desktop/data.txt")
read.table("~/Desktop/data.txt")
read.table("data.txt")
golf = read.table("~/Desktop/data.txt", header=TRUE, skip=2)
x = golf$x
y = golf$y
n = golf$n
J = length(y)
r = (1.68/2)/12
R = (4.25/2)/12
library("rstan")
printf(fit1)
sims1 = extract(fit1)
print(mean(sims1$sigma))
rm(ls())
rm(ls=ls())
rm(l=ls())
golf = read.table("~/Desktop/data.txt", header=TRUE, skip=2)
x = golf$x
y = golf$y
n = golf$n
J = length(y)
r = (1.68/2)/12
R = (4.25/2)/12
library("rstan")
printf(fit1)
library("rstan")
print(fit1)
fit1 = stan(file="~/Desktop/gcf.stan", dat="~/Desktop/data.txt")
fit1 = stan(file="~/Desktop/gcf.stan", dat="~/Desktop/data.txt")
fit1 = stan(file="~/Desktop/gcf.stan", dat="~/Desktop/data.txt")
golf = read.table("~/Desktop/data.txt", header=TRUE, skip=2)
x = golf$x
y = golf$y
n = golf$n
J = length(y)
r = (1.68/2)/12
R = (4.25/2)/12
library("rstan")
fit1 = stan(file="~/Desktop/gcf.stan", dat="~/Desktop/data.txt")
print(fit1)
fit = stan('normal.stan', data = data);
data = read_rdump('normal2.data.R');
fit = stan('normal.stan', data = data);
print(fit)
plot(fit)
library("rstan")
data = read_rdump('normal1.data.R');
library(rstan)
data = read_rdump('normal1.data.R');
fit = stan('normal.stan', data = data);
print(fit)
plot(fit)
library(rstan)
data = read_rdump("normal1.data.R");
fit = stan("normal.stan", data = data);
print(fit)
plot(fit)
library(rstan)
data = read_rdump("normal1.data.R");
fit = stan("normal.stan", data = data);
print(fit)
plot(fit)
load("~/Desktop/Fall2015/BDA/stan/stanTut/class3/normal.stan")
clear
library(rstan)
normal1 = read_rdump("normal1.data.R");
fit = stan("normal.stan", data = normal1);
print(fit)
plot(fit)
normal2 = read_rdump("normal2.data.R");
fit2 = stan("normal.stan", normal2 = data);
print(fit2)
plot(fit2)
normal2 <- read_rdump("normal2.data.R")
library(rstan)
normal1 <- read_rdump("normal1.data.R")
fit = stan("normal.stan", data = normal1)
print(fit)
plot(fit)
normal2 <- read_rdump("normal2.data.R")
fit2 = stan("normal.stan", normal2 = data)
print(fit2)
plot(fit2)
# normal1 <- read_rdump("normal1.data.R")
# fit = stan("normal.stan", data = normal1)
# print(fit)
# plot(fit)
#
# normal2 <- read_rdump("normal2.data.R")
# fit2 = stan("normal.stan", normal2 = data)
# print(fit2)
# plot(fit2)
library(rstan)
# normal1 <- read_rdump("normal1.data.R")
# fit = stan("normal.stan", data = normal1)
# print(fit)
# plot(fit)
#
# normal2 <- read_rdump("normal2.data.R")
# fit2 = stan("normal.stan", normal2 = data)
# print(fit2)
# plot(fit2)
library(rstan)
read_rdump("normal1.data.R")
source('~/Desktop/Fall2015/BDA/stan/stanTut/class3/tut.R')
library(rstan)
# normal1 <- read_rdump("normal1.data.R")
# fit = stan("normal.stan", data = normal1)
# print(fit)
# plot(fit)
#
# normal2 <- read_rdump("normal2.data.R")
# fit2 = stan("normal.stan", normal2 = data)
# print(fit2)
# plot(fit2)
library(rstan)
x=x+1
x=0
x=x+1
library(rstan)
normal1 <- read_rdump("normal1.data.R")
install.packages("topicmodels")
install.packages("RTextTools")
install.packages(c("RTextTools","topicmodels"))
install.packages("lda")
quit()
library(lda);
require("ggplot2")
require("reshape2")
# read input files
args <- commandArgs(trailingOnly = TRUE);
# docFile = args[1];
# vocabFile = args[2];
docFile = "../Tools/ap/ap.txt";
vocabFile = "../Tools/ap/vocab.txt";
doc = read.documents(docFile);
vocabs = read.vocab(vocabFile);
# cora = lexicalize(doc, sep = " ", lower = TRUE, count = 1L, vocab = NULL);
cora = lexicalize(doc, sep = " ", lower = TRUE,
count = 1L, vocab = vocabs);
data(cora.documents);
data(cora.vocab);
data(cora.titles);
# lda call
theme_set(theme_bw())
set.seed(8675309)
K = 50;
result <- lda.collapsed.gibbs.sampler(
cora.documents,K,cora.vocab,25,0.1,0.1,
compute.log.likelihood=TRUE);
## Get the top words in the cluster
top.words <- top.topic.words(result$topics, 5, by.score=TRUE);
## Number of documents to display
N <- 10
topic.proportions <- t(result$document_sums) / colSums(result$document_sums);
topic.proportions <- topic.proportions[sample(1:dim(topic.proportions)[1], N),];
topic.proportions[is.na(topic.proportions)] <-  1 / K;
colnames(topic.proportions) <- apply(top.words, 2, paste, collapse=" ");
topic.proportions.df <- melt(cbind(data.frame(topic.proportions),
document=factor(1:N)),
variable.name="topic",
id.vars = "document")
qplot(topic, value, fill=document, ylab="proportion",
data=topic.proportions.df, geom="bar", stat="identity") +
theme(axis.text.x = element_text(angle=90, hjust=1)) +
coord_flip() +
facet_wrap(~ document, ncol=5)
source('~/Cloud/Dropbox/Fall2015/GM-STATG6509/Proj/src/TMAnalysis.r', echo=TRUE)
source('~/Cloud/Dropbox/Fall2015/GM-STATG6509/Proj/src/TMAnalysis.r', echo=TRUE)
library(lda);
require("ggplot2")
require("reshape2")
# read input files
args <- commandArgs(trailingOnly = TRUE);
# docFile = args[1];
# vocabFile = args[2];
docFile = "../Tools/ap/ap.txt";
vocabFile = "../Tools/ap/vocab.txt";
doc = read.documents(docFile);
vocabs = read.vocab(vocabFile);
# cora = lexicalize(doc, sep = " ", lower = TRUE, count = 1L, vocab = NULL);
cora = lexicalize(doc, sep = " ", lower = TRUE,
count = 1L, vocab = vocabs);
data(cora.documents);
data(cora.vocab);
data(cora.titles);
# lda call
theme_set(theme_bw())
set.seed(8675309)
K = 50;
result <- lda.collapsed.gibbs.sampler(
cora.documents,K,cora.vocab,25,0.1,0.1,
compute.log.likelihood=TRUE);
## Get the top words in the cluster
top.words <- top.topic.words(result$topics, 5, by.score=TRUE);
## Number of documents to display
N <- 10
topic.proportions <- t(result$document_sums) / colSums(result$document_sums);
topic.proportions <- topic.proportions[sample(1:dim(topic.proportions)[1], N),];
topic.proportions[is.na(topic.proportions)] <-  1 / K;
colnames(topic.proportions) <- apply(top.words, 2, paste, collapse=" ");
topic.proportions.df <- melt(cbind(data.frame(topic.proportions),
document=factor(1:N)),
variable.name="topic",
id.vars = "document")
qplot(topic, value, fill=document, ylab="proportion",
data=topic.proportions.df, geom="bar", stat="identity") +
theme(axis.text.x = element_text(angle=90, hjust=1)) +
coord_flip() +
facet_wrap(~ document, ncol=5)
ls
source('~/Cloud/Dropbox/Fall2015/GM-STATG6509/Proj/src/TMAnalysis.r', echo=TRUE)
library(lda);
require("ggplot2")
require("reshape2")
# read input files
args <- commandArgs(trailingOnly = TRUE);
# docFile = args[1];
# vocabFile = args[2];
#docFile = "../Tools/ap/ap.txt";
#vocabFile = "../Tools/ap/vocab.txt";
doc = read.documents(docFile);
vocabs = read.vocab(vocabFile);
# cora = lexicalize(doc, sep = " ", lower = TRUE, count = 1L, vocab = NULL);
cora = lexicalize(doc, sep = " ", lower = TRUE,
count = 1L, vocab = vocabs);
data(cora.documents);
data(cora.vocab);
data(cora.titles);
# lda call
theme_set(theme_bw())
set.seed(8675309)
K = 50;
result <- lda.collapsed.gibbs.sampler(
cora.documents,K,cora.vocab,25,0.1,0.1,
compute.log.likelihood=TRUE);
## Get the top words in the cluster
top.words <- top.topic.words(result$topics, 5, by.score=TRUE);
## Number of documents to display
N <- 10
topic.proportions <- t(result$document_sums) / colSums(result$document_sums);
topic.proportions <- topic.proportions[sample(1:dim(topic.proportions)[1], N),];
topic.proportions[is.na(topic.proportions)] <-  1 / K;
colnames(topic.proportions) <- apply(top.words, 2, paste, collapse=" ");
topic.proportions.df <- melt(cbind(data.frame(topic.proportions),
document=factor(1:N)),
variable.name="topic",
id.vars = "document")
qplot(topic, value, fill=document, ylab="proportion",
data=topic.proportions.df, geom="bar", stat="identity") +
theme(axis.text.x = element_text(angle=90, hjust=1)) +
coord_flip() +
facet_wrap(~ document, ncol=5)
doc
file = "../Tools/ap/ap.txt"
read.documents(file)
read.documents(file,r)
rm(l=ls())
rm(list=ls())
cora = lexicalize(doc, sep = " ", lower = TRUE,
count = 1L, vocab = vocabs);
data(cora.documents);
data(cora.vocab);
data(cora.titles);
# lda call
theme_set(theme_bw())
set.seed(8675309)
K = 50;
result <- lda.collapsed.gibbs.sampler(
cora.documents,K,cora.vocab,25,0.1,0.1,
compute.log.likelihood=TRUE);
## Get the top words in the cluster
top.words <- top.topic.words(result$topics, 5, by.score=TRUE);
## Number of documents to display
N <- 10
topic.proportions <- t(result$document_sums) / colSums(result$document_sums);
topic.proportions <- topic.proportions[sample(1:dim(topic.proportions)[1], N),];
topic.proportions[is.na(topic.proportions)] <-  1 / K;
colnames(topic.proportions) <- apply(top.words, 2, paste, collapse=" ");
topic.proportions.df <- melt(cbind(data.frame(topic.proportions),
document=factor(1:N)),
variable.name="topic",
id.vars = "document")
qplot(topic, value, fill=document, ylab="proportion",
data=topic.proportions.df, geom="bar", stat="identity") +
theme(axis.text.x = element_text(angle=90, hjust=1)) +
coord_flip() +
facet_wrap(~ document, ncol=5)
rm(list=ls())
data(cora.documents);
cora
cora.documents
rm(list=ls())
data()?
;
setwd("~/Cloud/Dropbox/Fall2015/GM-STATG6509/Proj/src")
docFile = "../Tools/ap/ap.txt";
vocabFile = "../Tools/ap/vocab.txt";
doc = read.documents(docFile);
vocabs = read.vocab(vocabFile);
cora = lexicalize(doc, sep = " ", lower = TRUE,
count = 1L, vocab = vocabs);
warnings()
cora
docFile = "../Tools/ap/ap.txt";
vocabFile = "../Tools/ap/vocab.txt";
doc = read.documents(docFile);
vocabs = read.vocab(vocabFile);
cora = lexicalize(doc, sep = " ", lower = TRUE,
count = 1L, vocab = NULL);
doc
data("cora.documents", package = "lda")
data("cora.vocab", package = "lda")
dtm <- ldaformat2dtm(cora.documents, cora.vocab)
cora <- dtm2ldaformat(dtm)
data("cora.documents", package = "lda")
data("cora.vocab", package = "lda")
dtm <- ldaformat2dtm(cora.documents, cora.vocab)
cora <- dtm2ldaformat(dtm)
library(topicmodels)
cora <- dtm2ldaformat(dtm)
data("cora.documents", package = "lda")
data("cora.vocab", package = "lda")
dtm <- ldaformat2dtm(cora.documents, cora.vocab)
cora <- dtm2ldaformat(dtm)
dtm
rm(list=ls())
data("cora.documents", package = "lda")
data("cora.vocab", package = "lda")
dtm <- ldaformat2dtm(cora.documents, cora.vocab)
cora <- dtm2ldaformat(dtm)
cora
